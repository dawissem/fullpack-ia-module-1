{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONSMo+XiXQmaRPt0QwwEmx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dawissem/fullpack-ia-module-1/blob/main/nlp_process_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Projet NLP : Analyse et Description d'Images\n",
        "# √Ä ex√©cuter sur Google Colab\n",
        "\n",
        "# Installation des biblioth√®ques n√©cessaires\n",
        "!pip install transformers torch torchvision pillow gradio\n",
        "!pip install --upgrade transformers\n",
        "\n",
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import base64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AST-AOxA5aYQ",
        "outputId": "617850c9-1de5-4caa-9064-b478907706fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.14)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Configuration du device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device utilis√©: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3stOL1H68Qe",
        "outputId": "bcf39ddc-375b-49a7-81d9-cc2f6412ed7e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device utilis√©: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDescriptionModel:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialise le mod√®le de description d'images\"\"\"\n",
        "        print(\"Chargement du mod√®le BLIP...\")\n",
        "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        self.model.to(device)\n",
        "        print(\"Mod√®le charg√© avec succ√®s!\")\n",
        "\n",
        "    def analyze_image(self, image):\n",
        "        \"\"\"Analyse une image et g√©n√®re une description\"\"\"\n",
        "        try:\n",
        "            # Pr√©paration de l'image\n",
        "            if isinstance(image, str):\n",
        "                # Si c'est un chemin de fichier\n",
        "                image = Image.open(image).convert('RGB')\n",
        "            elif isinstance(image, np.ndarray):\n",
        "                # Si c'est un array numpy\n",
        "                image = Image.fromarray(image).convert('RGB')\n",
        "\n",
        "            # Traitement de l'image\n",
        "            inputs = self.processor(image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            # G√©n√©ration de la description\n",
        "            with torch.no_grad():\n",
        "                out = self.model.generate(**inputs, max_length=100, num_beams=5)\n",
        "\n",
        "            # D√©codage du texte\n",
        "            description = self.processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "            return description\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Erreur lors de l'analyse: {str(e)}\"\n",
        "\n",
        "    def analyze_with_context(self, image, context_prompt=\"\"):\n",
        "        \"\"\"Analyse une image avec un contexte sp√©cifique\"\"\"\n",
        "        try:\n",
        "            if isinstance(image, str):\n",
        "                image = Image.open(image).convert('RGB')\n",
        "            elif isinstance(image, np.ndarray):\n",
        "                image = Image.fromarray(image).convert('RGB')\n",
        "\n",
        "            # Utilisation d'un prompt conditionnel si fourni\n",
        "            if context_prompt:\n",
        "                inputs = self.processor(image, context_prompt, return_tensors=\"pt\").to(device)\n",
        "            else:\n",
        "                inputs = self.processor(image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                out = self.model.generate(**inputs, max_length=150, num_beams=5)\n",
        "\n",
        "            description = self.processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "            # Nettoyer la description si elle contient le prompt\n",
        "            if context_prompt and description.startswith(context_prompt):\n",
        "                description = description[len(context_prompt):].strip()\n",
        "\n",
        "            return description\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Erreur lors de l'analyse contextuelle: {str(e)}\""
      ],
      "metadata": {
        "id": "NPK9qC5U7D6f"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "O-Ld6NH85WKM"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ImageAnalyzer:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialise l'analyseur d'images\"\"\"\n",
        "        self.model = ImageDescriptionModel()\n",
        "\n",
        "    def full_analysis(self, image, include_context=True):\n",
        "        \"\"\"Effectue une analyse compl√®te de l'image\"\"\"\n",
        "        results = {\n",
        "            \"description_generale\": \"\",\n",
        "            \"description_detaillee\": \"\",\n",
        "            \"elements_detectes\": \"\",\n",
        "            \"couleurs_dominantes\": \"\",\n",
        "            \"style_artistic\": \"\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Description g√©n√©rale\n",
        "            results[\"description_generale\"] = self.model.analyze_image(image)\n",
        "\n",
        "            if include_context:\n",
        "                # Description d√©taill√©e\n",
        "                results[\"description_detaillee\"] = self.model.analyze_with_context(\n",
        "                    image, \"D√©cris cette image en d√©tail:\"\n",
        "                )\n",
        "\n",
        "                # √âl√©ments d√©tect√©s\n",
        "                results[\"elements_detectes\"] = self.model.analyze_with_context(\n",
        "                    image, \"Quels objets vois-tu dans cette image?\"\n",
        "                )\n",
        "\n",
        "                # Analyse des couleurs\n",
        "                results[\"couleurs_dominantes\"] = self.analyze_colors(image)\n",
        "\n",
        "                # Style artistique\n",
        "                results[\"style_artistic\"] = self.model.analyze_with_context(\n",
        "                    image, \"Quel est le style artistique de cette image?\"\n",
        "                )\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\"erreur\": f\"Erreur lors de l'analyse compl√®te: {str(e)}\"}\n",
        "\n",
        "    def analyze_colors(self, image):\n",
        "        \"\"\"Analyse les couleurs dominantes de l'image\"\"\"\n",
        "        try:\n",
        "            if isinstance(image, str):\n",
        "                img = Image.open(image).convert('RGB')\n",
        "            elif isinstance(image, np.ndarray):\n",
        "                img = Image.fromarray(image).convert('RGB')\n",
        "            else:\n",
        "                img = image\n",
        "\n",
        "            # Redimensionner pour l'analyse\n",
        "            img = img.resize((150, 150))\n",
        "\n",
        "            # Convertir en array numpy\n",
        "            img_array = np.array(img)\n",
        "\n",
        "            # Calculer les couleurs moyennes\n",
        "            avg_color = np.mean(img_array, axis=(0, 1))\n",
        "\n",
        "            # D√©terminer les couleurs dominantes\n",
        "            colors = []\n",
        "            if avg_color[0] > avg_color[1] and avg_color[0] > avg_color[2]:\n",
        "                colors.append(\"rouge\")\n",
        "            if avg_color[1] > avg_color[0] and avg_color[1] > avg_color[2]:\n",
        "                colors.append(\"vert\")\n",
        "            if avg_color[2] > avg_color[0] and avg_color[2] > avg_color[1]:\n",
        "                colors.append(\"bleu\")\n",
        "\n",
        "            if not colors:\n",
        "                if np.mean(avg_color) > 200:\n",
        "                    colors.append(\"clair\")\n",
        "                elif np.mean(avg_color) < 100:\n",
        "                    colors.append(\"sombre\")\n",
        "                else:\n",
        "                    colors.append(\"neutre\")\n",
        "\n",
        "            return f\"Couleurs dominantes: {', '.join(colors)}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Erreur analyse couleurs: {str(e)}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gradio_interface():\n",
        "    \"\"\"Cr√©e l'interface Gradio pour l'application\"\"\"\n",
        "\n",
        "    # Initialiser l'analyseur\n",
        "    analyzer = ImageAnalyzer()\n",
        "\n",
        "    def process_image(image, analysis_type, custom_prompt=\"\"):\n",
        "        \"\"\"Traite l'image selon le type d'analyse demand√©\"\"\"\n",
        "        if image is None:\n",
        "            return \"Veuillez t√©l√©charger une image.\"\n",
        "\n",
        "        try:\n",
        "            if analysis_type == \"Description simple\":\n",
        "                result = analyzer.model.analyze_image(image)\n",
        "                return f\"Description: {result}\"\n",
        "\n",
        "            elif analysis_type == \"Description avec contexte\":\n",
        "                if custom_prompt:\n",
        "                    result = analyzer.model.analyze_with_context(image, custom_prompt)\n",
        "                    return f\"Description contextuelle: {result}\"\n",
        "                else:\n",
        "                    return \"Veuillez fournir un prompt pour l'analyse contextuelle.\"\n",
        "\n",
        "            elif analysis_type == \"Analyse compl√®te\":\n",
        "                results = analyzer.full_analysis(image)\n",
        "\n",
        "                output = \"=== ANALYSE COMPL√àTE ===\\n\\n\"\n",
        "                for key, value in results.items():\n",
        "                    if key != \"erreur\":\n",
        "                        output += f\"{key.replace('_', ' ').title()}: {value}\\n\\n\"\n",
        "\n",
        "                return output\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Erreur lors du traitement: {str(e)}\"\n",
        "\n",
        "    # Interface Gradio\n",
        "    with gr.Blocks(title=\"Analyseur d'Images NLP\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"# üñºÔ∏è Analyseur d'Images avec NLP\")\n",
        "        gr.Markdown(\"T√©l√©chargez une image et obtenez une description d√©taill√©e g√©n√©r√©e par IA.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                image_input = gr.Image(\n",
        "                    label=\"T√©l√©chargez votre image\",\n",
        "                    type=\"pil\"\n",
        "                )\n",
        "\n",
        "                analysis_type = gr.Dropdown(\n",
        "                    choices=[\"Description simple\", \"Description avec contexte\", \"Analyse compl√®te\"],\n",
        "                    value=\"Description simple\",\n",
        "                    label=\"Type d'analyse\"\n",
        "                )\n",
        "\n",
        "                custom_prompt = gr.Textbox(\n",
        "                    label=\"Prompt personnalis√© (optionnel)\",\n",
        "                    placeholder=\"Ex: D√©cris les √©motions dans cette image\",\n",
        "                    visible=False\n",
        "                )\n",
        "\n",
        "                analyze_btn = gr.Button(\"Analyser l'image\", variant=\"primary\")\n",
        "\n",
        "                # Montrer/cacher le prompt personnalis√©\n",
        "                def toggle_prompt(choice):\n",
        "                    return gr.update(visible=(choice == \"Description avec contexte\"))\n",
        "\n",
        "                analysis_type.change(toggle_prompt, analysis_type, custom_prompt)\n",
        "\n",
        "            with gr.Column():\n",
        "                output_text = gr.Textbox(\n",
        "                    label=\"R√©sultat de l'analyse\",\n",
        "                    lines=15,\n",
        "                    max_lines=20\n",
        "                )\n",
        "\n",
        "        # Exemples d'images\n",
        "        gr.Markdown(\"### Exemples d'images √† tester:\")\n",
        "        example_images = [\n",
        "            [\"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400\"],\n",
        "            [\"https://images.unsplash.com/photo-1551963831-b3b1ca40c98e?w=400\"],\n",
        "            [\"https://images.unsplash.com/photo-1518717758536-85ae29035b6d?w=400\"]\n",
        "        ]\n",
        "\n",
        "        gr.Examples(\n",
        "            examples=example_images,\n",
        "            inputs=image_input,\n",
        "            label=\"Cliquez sur une image d'exemple\"\n",
        "        )\n",
        "\n",
        "        # Connecter les √©v√©nements\n",
        "        analyze_btn.click(\n",
        "            process_image,\n",
        "            inputs=[image_input, analysis_type, custom_prompt],\n",
        "            outputs=output_text\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Fonction principale\n",
        "def main():\n",
        "    \"\"\"Fonction principale pour lancer l'application\"\"\"\n",
        "    print(\"Initialisation de l'application...\")\n",
        "\n",
        "    # Cr√©er et lancer l'interface\n",
        "    demo = create_gradio_interface()\n",
        "\n",
        "    print(\"Lancement de l'interface Gradio...\")\n",
        "    demo.launch(\n",
        "        share=True,  # Permet de partager l'interface\n",
        "        debug=True,\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7860\n",
        "    )\n",
        "\n",
        "# Instructions d'utilisation\n",
        "print(\"\"\"\n",
        "=== INSTRUCTIONS D'UTILISATION ===\n",
        "\n",
        "1. Ex√©cutez toutes les cellules dans l'ordre\n",
        "2. Attendez que les mod√®les se chargent\n",
        "3. Utilisez l'interface Gradio qui s'ouvre\n",
        "4. T√©l√©chargez une image et choisissez le type d'analyse\n",
        "5. Cliquez sur \"Analyser l'image\"\n",
        "\n",
        "Types d'analyse disponibles:\n",
        "- Description simple: Description g√©n√©rale de l'image\n",
        "- Description avec contexte: Description avec un prompt personnalis√©\n",
        "- Analyse compl√®te: Analyse d√©taill√©e avec plusieurs aspects\n",
        "\n",
        "Pour lancer l'application, ex√©cutez:\n",
        "main()\n",
        "\"\"\")\n",
        "\n",
        "# D√©commenter la ligne suivante pour lancer automatiquement\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0EyEymb07LYZ",
        "outputId": "fb2cc30e-b1c0-43f6-ab91-92913f5d4564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== INSTRUCTIONS D'UTILISATION ===\n",
            "\n",
            "1. Ex√©cutez toutes les cellules dans l'ordre\n",
            "2. Attendez que les mod√®les se chargent\n",
            "3. Utilisez l'interface Gradio qui s'ouvre\n",
            "4. T√©l√©chargez une image et choisissez le type d'analyse\n",
            "5. Cliquez sur \"Analyser l'image\"\n",
            "\n",
            "Types d'analyse disponibles:\n",
            "- Description simple: Description g√©n√©rale de l'image\n",
            "- Description avec contexte: Description avec un prompt personnalis√©\n",
            "- Analyse compl√®te: Analyse d√©taill√©e avec plusieurs aspects\n",
            "\n",
            "Pour lancer l'application, ex√©cutez:\n",
            "main()\n",
            "\n",
            "Initialisation de l'application...\n",
            "Chargement du mod√®le BLIP...\n",
            "Mod√®le charg√© avec succ√®s!\n",
            "Lancement de l'interface Gradio...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://aad3a6f417e7a7311f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://aad3a6f417e7a7311f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3570: UserWarning: image file could not be identified because AVIF support not installed\n",
            "  warnings.warn(message)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2187, in process_api\n",
            "    inputs = await self.preprocess_data(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1853, in preprocess_data\n",
            "    processed_input.append(block.preprocess(inputs_cached))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/components/image.py\", line 218, in preprocess\n",
            "    return image_utils.preprocess_image(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/image_utils.py\", line 218, in preprocess_image\n",
            "    im = PIL.Image.open(file_path)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3572, in open\n",
            "    raise UnidentifiedImageError(msg)\n",
            "PIL.UnidentifiedImageError: cannot identify image file '/tmp/gradio/6f61818f2b72f26dd6d1478e1090182f710d9fe4df48107caa3ed81e368db375/drawing-person-with-sad-expression_1140815-2172.avif'\n"
          ]
        }
      ]
    }
  ]
}