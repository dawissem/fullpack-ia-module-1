{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONSMo+XiXQmaRPt0QwwEmx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dawissem/fullpack-ia-module-1/blob/main/nlp_process_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Projet NLP : Analyse et Description d'Images\n",
        "# À exécuter sur Google Colab\n",
        "\n",
        "# Installation des bibliothèques nécessaires\n",
        "!pip install transformers torch torchvision pillow gradio\n",
        "!pip install --upgrade transformers\n",
        "\n",
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import base64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AST-AOxA5aYQ",
        "outputId": "617850c9-1de5-4caa-9064-b478907706fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.14)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Configuration du device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device utilisé: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3stOL1H68Qe",
        "outputId": "bcf39ddc-375b-49a7-81d9-cc2f6412ed7e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device utilisé: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDescriptionModel:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialise le modèle de description d'images\"\"\"\n",
        "        print(\"Chargement du modèle BLIP...\")\n",
        "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        self.model.to(device)\n",
        "        print(\"Modèle chargé avec succès!\")\n",
        "\n",
        "    def analyze_image(self, image):\n",
        "        \"\"\"Analyse une image et génère une description\"\"\"\n",
        "        try:\n",
        "            # Préparation de l'image\n",
        "            if isinstance(image, str):\n",
        "                # Si c'est un chemin de fichier\n",
        "                image = Image.open(image).convert('RGB')\n",
        "            elif isinstance(image, np.ndarray):\n",
        "                # Si c'est un array numpy\n",
        "                image = Image.fromarray(image).convert('RGB')\n",
        "\n",
        "            # Traitement de l'image\n",
        "            inputs = self.processor(image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            # Génération de la description\n",
        "            with torch.no_grad():\n",
        "                out = self.model.generate(**inputs, max_length=100, num_beams=5)\n",
        "\n",
        "            # Décodage du texte\n",
        "            description = self.processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "            return description\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Erreur lors de l'analyse: {str(e)}\"\n",
        "\n",
        "    def analyze_with_context(self, image, context_prompt=\"\"):\n",
        "        \"\"\"Analyse une image avec un contexte spécifique\"\"\"\n",
        "        try:\n",
        "            if isinstance(image, str):\n",
        "                image = Image.open(image).convert('RGB')\n",
        "            elif isinstance(image, np.ndarray):\n",
        "                image = Image.fromarray(image).convert('RGB')\n",
        "\n",
        "            # Utilisation d'un prompt conditionnel si fourni\n",
        "            if context_prompt:\n",
        "                inputs = self.processor(image, context_prompt, return_tensors=\"pt\").to(device)\n",
        "            else:\n",
        "                inputs = self.processor(image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                out = self.model.generate(**inputs, max_length=150, num_beams=5)\n",
        "\n",
        "            description = self.processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "            # Nettoyer la description si elle contient le prompt\n",
        "            if context_prompt and description.startswith(context_prompt):\n",
        "                description = description[len(context_prompt):].strip()\n",
        "\n",
        "            return description\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Erreur lors de l'analyse contextuelle: {str(e)}\""
      ],
      "metadata": {
        "id": "NPK9qC5U7D6f"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "O-Ld6NH85WKM"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ImageAnalyzer:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialise l'analyseur d'images\"\"\"\n",
        "        self.model = ImageDescriptionModel()\n",
        "\n",
        "    def full_analysis(self, image, include_context=True):\n",
        "        \"\"\"Effectue une analyse complète de l'image\"\"\"\n",
        "        results = {\n",
        "            \"description_generale\": \"\",\n",
        "            \"description_detaillee\": \"\",\n",
        "            \"elements_detectes\": \"\",\n",
        "            \"couleurs_dominantes\": \"\",\n",
        "            \"style_artistic\": \"\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Description générale\n",
        "            results[\"description_generale\"] = self.model.analyze_image(image)\n",
        "\n",
        "            if include_context:\n",
        "                # Description détaillée\n",
        "                results[\"description_detaillee\"] = self.model.analyze_with_context(\n",
        "                    image, \"Décris cette image en détail:\"\n",
        "                )\n",
        "\n",
        "                # Éléments détectés\n",
        "                results[\"elements_detectes\"] = self.model.analyze_with_context(\n",
        "                    image, \"Quels objets vois-tu dans cette image?\"\n",
        "                )\n",
        "\n",
        "                # Analyse des couleurs\n",
        "                results[\"couleurs_dominantes\"] = self.analyze_colors(image)\n",
        "\n",
        "                # Style artistique\n",
        "                results[\"style_artistic\"] = self.model.analyze_with_context(\n",
        "                    image, \"Quel est le style artistique de cette image?\"\n",
        "                )\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\"erreur\": f\"Erreur lors de l'analyse complète: {str(e)}\"}\n",
        "\n",
        "    def analyze_colors(self, image):\n",
        "        \"\"\"Analyse les couleurs dominantes de l'image\"\"\"\n",
        "        try:\n",
        "            if isinstance(image, str):\n",
        "                img = Image.open(image).convert('RGB')\n",
        "            elif isinstance(image, np.ndarray):\n",
        "                img = Image.fromarray(image).convert('RGB')\n",
        "            else:\n",
        "                img = image\n",
        "\n",
        "            # Redimensionner pour l'analyse\n",
        "            img = img.resize((150, 150))\n",
        "\n",
        "            # Convertir en array numpy\n",
        "            img_array = np.array(img)\n",
        "\n",
        "            # Calculer les couleurs moyennes\n",
        "            avg_color = np.mean(img_array, axis=(0, 1))\n",
        "\n",
        "            # Déterminer les couleurs dominantes\n",
        "            colors = []\n",
        "            if avg_color[0] > avg_color[1] and avg_color[0] > avg_color[2]:\n",
        "                colors.append(\"rouge\")\n",
        "            if avg_color[1] > avg_color[0] and avg_color[1] > avg_color[2]:\n",
        "                colors.append(\"vert\")\n",
        "            if avg_color[2] > avg_color[0] and avg_color[2] > avg_color[1]:\n",
        "                colors.append(\"bleu\")\n",
        "\n",
        "            if not colors:\n",
        "                if np.mean(avg_color) > 200:\n",
        "                    colors.append(\"clair\")\n",
        "                elif np.mean(avg_color) < 100:\n",
        "                    colors.append(\"sombre\")\n",
        "                else:\n",
        "                    colors.append(\"neutre\")\n",
        "\n",
        "            return f\"Couleurs dominantes: {', '.join(colors)}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Erreur analyse couleurs: {str(e)}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gradio_interface():\n",
        "    \"\"\"Crée l'interface Gradio pour l'application\"\"\"\n",
        "\n",
        "    # Initialiser l'analyseur\n",
        "    analyzer = ImageAnalyzer()\n",
        "\n",
        "    def process_image(image, analysis_type, custom_prompt=\"\"):\n",
        "        \"\"\"Traite l'image selon le type d'analyse demandé\"\"\"\n",
        "        if image is None:\n",
        "            return \"Veuillez télécharger une image.\"\n",
        "\n",
        "        try:\n",
        "            if analysis_type == \"Description simple\":\n",
        "                result = analyzer.model.analyze_image(image)\n",
        "                return f\"Description: {result}\"\n",
        "\n",
        "            elif analysis_type == \"Description avec contexte\":\n",
        "                if custom_prompt:\n",
        "                    result = analyzer.model.analyze_with_context(image, custom_prompt)\n",
        "                    return f\"Description contextuelle: {result}\"\n",
        "                else:\n",
        "                    return \"Veuillez fournir un prompt pour l'analyse contextuelle.\"\n",
        "\n",
        "            elif analysis_type == \"Analyse complète\":\n",
        "                results = analyzer.full_analysis(image)\n",
        "\n",
        "                output = \"=== ANALYSE COMPLÈTE ===\\n\\n\"\n",
        "                for key, value in results.items():\n",
        "                    if key != \"erreur\":\n",
        "                        output += f\"{key.replace('_', ' ').title()}: {value}\\n\\n\"\n",
        "\n",
        "                return output\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Erreur lors du traitement: {str(e)}\"\n",
        "\n",
        "    # Interface Gradio\n",
        "    with gr.Blocks(title=\"Analyseur d'Images NLP\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"# 🖼️ Analyseur d'Images avec NLP\")\n",
        "        gr.Markdown(\"Téléchargez une image et obtenez une description détaillée générée par IA.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                image_input = gr.Image(\n",
        "                    label=\"Téléchargez votre image\",\n",
        "                    type=\"pil\"\n",
        "                )\n",
        "\n",
        "                analysis_type = gr.Dropdown(\n",
        "                    choices=[\"Description simple\", \"Description avec contexte\", \"Analyse complète\"],\n",
        "                    value=\"Description simple\",\n",
        "                    label=\"Type d'analyse\"\n",
        "                )\n",
        "\n",
        "                custom_prompt = gr.Textbox(\n",
        "                    label=\"Prompt personnalisé (optionnel)\",\n",
        "                    placeholder=\"Ex: Décris les émotions dans cette image\",\n",
        "                    visible=False\n",
        "                )\n",
        "\n",
        "                analyze_btn = gr.Button(\"Analyser l'image\", variant=\"primary\")\n",
        "\n",
        "                # Montrer/cacher le prompt personnalisé\n",
        "                def toggle_prompt(choice):\n",
        "                    return gr.update(visible=(choice == \"Description avec contexte\"))\n",
        "\n",
        "                analysis_type.change(toggle_prompt, analysis_type, custom_prompt)\n",
        "\n",
        "            with gr.Column():\n",
        "                output_text = gr.Textbox(\n",
        "                    label=\"Résultat de l'analyse\",\n",
        "                    lines=15,\n",
        "                    max_lines=20\n",
        "                )\n",
        "\n",
        "        # Exemples d'images\n",
        "        gr.Markdown(\"### Exemples d'images à tester:\")\n",
        "        example_images = [\n",
        "            [\"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400\"],\n",
        "            [\"https://images.unsplash.com/photo-1551963831-b3b1ca40c98e?w=400\"],\n",
        "            [\"https://images.unsplash.com/photo-1518717758536-85ae29035b6d?w=400\"]\n",
        "        ]\n",
        "\n",
        "        gr.Examples(\n",
        "            examples=example_images,\n",
        "            inputs=image_input,\n",
        "            label=\"Cliquez sur une image d'exemple\"\n",
        "        )\n",
        "\n",
        "        # Connecter les événements\n",
        "        analyze_btn.click(\n",
        "            process_image,\n",
        "            inputs=[image_input, analysis_type, custom_prompt],\n",
        "            outputs=output_text\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Fonction principale\n",
        "def main():\n",
        "    \"\"\"Fonction principale pour lancer l'application\"\"\"\n",
        "    print(\"Initialisation de l'application...\")\n",
        "\n",
        "    # Créer et lancer l'interface\n",
        "    demo = create_gradio_interface()\n",
        "\n",
        "    print(\"Lancement de l'interface Gradio...\")\n",
        "    demo.launch(\n",
        "        share=True,  # Permet de partager l'interface\n",
        "        debug=True,\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7860\n",
        "    )\n",
        "\n",
        "# Instructions d'utilisation\n",
        "print(\"\"\"\n",
        "=== INSTRUCTIONS D'UTILISATION ===\n",
        "\n",
        "1. Exécutez toutes les cellules dans l'ordre\n",
        "2. Attendez que les modèles se chargent\n",
        "3. Utilisez l'interface Gradio qui s'ouvre\n",
        "4. Téléchargez une image et choisissez le type d'analyse\n",
        "5. Cliquez sur \"Analyser l'image\"\n",
        "\n",
        "Types d'analyse disponibles:\n",
        "- Description simple: Description générale de l'image\n",
        "- Description avec contexte: Description avec un prompt personnalisé\n",
        "- Analyse complète: Analyse détaillée avec plusieurs aspects\n",
        "\n",
        "Pour lancer l'application, exécutez:\n",
        "main()\n",
        "\"\"\")\n",
        "\n",
        "# Décommenter la ligne suivante pour lancer automatiquement\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0EyEymb07LYZ",
        "outputId": "fb2cc30e-b1c0-43f6-ab91-92913f5d4564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== INSTRUCTIONS D'UTILISATION ===\n",
            "\n",
            "1. Exécutez toutes les cellules dans l'ordre\n",
            "2. Attendez que les modèles se chargent\n",
            "3. Utilisez l'interface Gradio qui s'ouvre\n",
            "4. Téléchargez une image et choisissez le type d'analyse\n",
            "5. Cliquez sur \"Analyser l'image\"\n",
            "\n",
            "Types d'analyse disponibles:\n",
            "- Description simple: Description générale de l'image\n",
            "- Description avec contexte: Description avec un prompt personnalisé\n",
            "- Analyse complète: Analyse détaillée avec plusieurs aspects\n",
            "\n",
            "Pour lancer l'application, exécutez:\n",
            "main()\n",
            "\n",
            "Initialisation de l'application...\n",
            "Chargement du modèle BLIP...\n",
            "Modèle chargé avec succès!\n",
            "Lancement de l'interface Gradio...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://aad3a6f417e7a7311f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://aad3a6f417e7a7311f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3570: UserWarning: image file could not be identified because AVIF support not installed\n",
            "  warnings.warn(message)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2187, in process_api\n",
            "    inputs = await self.preprocess_data(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1853, in preprocess_data\n",
            "    processed_input.append(block.preprocess(inputs_cached))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/components/image.py\", line 218, in preprocess\n",
            "    return image_utils.preprocess_image(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/image_utils.py\", line 218, in preprocess_image\n",
            "    im = PIL.Image.open(file_path)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3572, in open\n",
            "    raise UnidentifiedImageError(msg)\n",
            "PIL.UnidentifiedImageError: cannot identify image file '/tmp/gradio/6f61818f2b72f26dd6d1478e1090182f710d9fe4df48107caa3ed81e368db375/drawing-person-with-sad-expression_1140815-2172.avif'\n"
          ]
        }
      ]
    }
  ]
}